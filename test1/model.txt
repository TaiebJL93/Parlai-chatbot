12/02/2021 11:24:05 PM: [ COMMAND: retrieval_train.py --batch-size 32 --bert-dim 300 --cuda --dataset-name empchat --dict-max-words 250000 --display-iter 100 --embeddings None --empchat-folder ./empatheticdialogues/ --max-hist-len 4 --model bert --model-dir ./test1 --model-name model --optimizer adamax --pretrained ./bert_finetuned_emoprepend1.mdl --reactonly ]
12/02/2021 11:24:05 PM: [ ---------------------------------------------------------------------------------------------------- ]
12/02/2021 11:24:05 PM: [ CONFIG:
{
    "batch_size": 32,
    "bert_add_transformer_layer": false,
    "bert_dim": 300,
    "cuda": false,
    "dailydialog_folder": null,
    "dataset_name": "empchat",
    "dict_max_words": 250000,
    "display_iter": 100,
    "embeddings": "None",
    "embeddings_size": 300,
    "empchat_folder": "./empatheticdialogues/",
    "epoch_start": 0,
    "fasttext": null,
    "fasttext_path": null,
    "fasttext_type": null,
    "hits_at_nb_cands": 100,
    "learn_embeddings": false,
    "learning_rate": null,
    "load_checkpoint": null,
    "log_file": "./test1/model.txt",
    "max_hist_len": 4,
    "max_sent_len": 100,
    "model": "bert",
    "model_dir": "./test1",
    "model_file": "./test1/model.mdl",
    "model_name": "model",
    "n_layers": 6,
    "no_shuffle": false,
    "normalize_emb": false,
    "normalize_sent_emb": false,
    "num_epochs": 1000,
    "optimizer": "adamax",
    "pretrained": "./bert_finetuned_emoprepend1.mdl",
    "random_seed": 92179,
    "reactonly": true,
    "reddit_folder": null,
    "rm_long_contexts": false,
    "rm_long_sent": false,
    "stop_crit_num_epochs": -1,
    "transformer_dim": 512,
    "transformer_dropout": 0,
    "transformer_n_heads": 8
} ]
12/02/2021 11:24:05 PM: [ Loading model ./bert_finetuned_emoprepend1.mdl ]
12/04/2021 12:26:05 AM: [ COMMAND: retrieval_train.py --batch-size 32 --bert-dim 300 --cuda --dataset-name empchat --dict-max-words 250000 --display-iter 100 --embeddings None --empchat-folder ./empatheticdialogues/ --max-hist-len 4 --model bert --model-dir ./test1 --model-name model --optimizer adamax --pretrained ./bert_finetuned_emoprepend1.mdl --reactonly ]
12/04/2021 12:26:05 AM: [ ---------------------------------------------------------------------------------------------------- ]
12/04/2021 12:26:05 AM: [ CONFIG:
{
    "batch_size": 32,
    "bert_add_transformer_layer": false,
    "bert_dim": 300,
    "cuda": false,
    "dailydialog_folder": null,
    "dataset_name": "empchat",
    "dict_max_words": 250000,
    "display_iter": 100,
    "embeddings": "None",
    "embeddings_size": 300,
    "empchat_folder": "./empatheticdialogues/",
    "epoch_start": 0,
    "fasttext": null,
    "fasttext_path": null,
    "fasttext_type": null,
    "hits_at_nb_cands": 100,
    "learn_embeddings": false,
    "learning_rate": null,
    "load_checkpoint": null,
    "log_file": "./test1/model.txt",
    "max_hist_len": 4,
    "max_sent_len": 100,
    "model": "bert",
    "model_dir": "./test1",
    "model_file": "./test1/model.mdl",
    "model_name": "model",
    "n_layers": 6,
    "no_shuffle": false,
    "normalize_emb": false,
    "normalize_sent_emb": false,
    "num_epochs": 1000,
    "optimizer": "adamax",
    "pretrained": "./bert_finetuned_emoprepend1.mdl",
    "random_seed": 92179,
    "reactonly": true,
    "reddit_folder": null,
    "rm_long_contexts": false,
    "rm_long_sent": false,
    "stop_crit_num_epochs": -1,
    "transformer_dim": 512,
    "transformer_dropout": 0,
    "transformer_n_heads": 8
} ]
12/04/2021 12:26:05 AM: [ Loading model ./bert_finetuned_emoprepend1.mdl ]
12/04/2021 12:26:12 AM: [ Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex . ]
